---
title: "Assessment 3: Group Project"
team: "Team Echo"
session: "Hexa 2, 2024"
coursecode: "ZZSC9020"
authors:
  - "Chuang, Keith – z5449930"
  - "Gandhi, Rupesh – z5368767"
  - "Melbin, Joseph – z5394849"
  - "CHIU, Yiu Tong – z5039191"
date: "2024-04-20"

Acknowledgements: 
  - "TBC."
  - "TBC."
  - "TBC."
  - "TBC."

output: html_document

---

# Abstract

---

# Contents

---

# 1.0 Introduction

---

# Packages

```

install.packages("readr")
install.packages("here")
install.packages("ggplot2")
install.packages("tidyverse")
install.packages("lubridate")
install.packages("mgcv")
install.packages("fpp2")
install.packages("fable")
install.packages('feasts')
install.packages('corrplot')
install.packages("corrplot")

```


```{r, message=FALSE}
library(ggplot2)
library(tidyverse)
library(lubridate)
library(mgcv)
library(fpp3)
library(fable)
library(fabletools)
library(feasts)
library(corrplot)
library(forecast)
library(reshape2)
library(tseries)
library(tsibble)
library(doParallel)
library(zoo)
library(caret)
library(lattice)
library(pdp)
library(foreach)
library(furrr)
```


---

# 4.0 Exploratory Data Analysis

## 4.1 Area of Research
Determine the efficacy of temperature as the independent variable in prediction 
of energy demand in the subsequent 24 hours on a half hourly basis for NSW 
utilizing statistical models i.e. regression. Explore the predictive improvement 
through the addition of independent variables. Do these models provide 
comparable predictive results to the supplied predictions and if so under what 
circumstances.

## Collecting and Loading Data

### Provided Data 

```{r, message=FALSE}

# NSW
data_temperature_nsw <- read_csv(unzip("../data/NSW/temperature_nsw.csv.zip", files = "temperature_nsw.csv", exdir = tempdir())[1])
data_demand_nsw <- read_csv(unzip("../data/NSW/totaldemand_nsw.csv.zip", files = "totaldemand_nsw.csv", exdir = tempdir())[1])

```

```{r ,message=FALSE}
data_forecastdemand_nsw_part1 <- read_csv(unzip("../data/NSW/forecastdemand/forecastdemand_part1.csv.zip", files = "forecastdemand_part1.csv", exdir = tempdir())[1])

data_forecastdemand_nsw_part2 <- read_csv(unzip("../data/NSW/forecastdemand/forecastdemand_part2.csv.zip", files = "forecastdemand_part2.csv", exdir = tempdir())[1])

data_forecastdemand_nsw_part3 <- read_csv(unzip("../data/NSW/forecastdemand/forecastdemand_part3.csv.zip", files = "forecastdemand_part3.csv", exdir = tempdir())[1])

data_forecastdemand_nsw_part4 <- read_csv(unzip("../data/NSW/forecastdemand/forecastdemand_part4.csv.zip", files = "forecastdemand_part4.csv", exdir = tempdir())[1])

data_forecastdemand_nsw_part5 <- read_csv(unzip("../data/NSW/forecastdemand/forecastdemand_part5.csv.zip", files = "forecastdemand_part5.csv", exdir = tempdir())[1])

```

```{r}
# Combine all forecastdemand into 1 df

data_forecastdemand_nsw <- bind_rows(data_forecastdemand_nsw_part1,
                                     data_forecastdemand_nsw_part2,
                                     data_forecastdemand_nsw_part3,
                                     data_forecastdemand_nsw_part4,
                                     data_forecastdemand_nsw_part5) 

# Clear memory
rm(data_forecastdemand_nsw_part1,
   data_forecastdemand_nsw_part2,
   data_forecastdemand_nsw_part3,
   data_forecastdemand_nsw_part4,
   data_forecastdemand_nsw_part5)

```


### 4.2.2 Sourced Data

```{r, message=FALSE}
## Reference: https://www.michaelplazzer.com/datasets/australian-public-holiday-data/

holidays <- read_csv('../data/Aus_public_hols_2009-2022-1.csv')

holidays_nsw <- holidays %>% 
  filter(State == 'NSW')


## Reference: http://www.bom.gov.au/climate/data/index.shtml
solar_expo_nsw <- read_csv("../data/daily_solar_exposure_bankstown_airport/IDCJAC0016_066137_1800_Data.csv")

## Reference: http://www.bom.gov.au/climate/data/index.shtml
rainfall_nsw <- read_csv("../data/daily_rainfall_bankstown_airport/IDCJAC0009_066137_1800_Data.csv")

## Reference: https://data.worldbank.org/indicator/NY.GDP.MKTP.CD?locations=AU
gdp <- read_csv('../data/GDP.csv', skip=4) %>% filter(`Country Name` == 'Australia')

```


##### Functions to clean and wrangle data
``` {r}
# Checks for NA values

check_for_na <- function(data) {
  # Get the name of the dataframe
  data_name <- deparse(substitute(data))
  
  # Print the name of the dataframe
  cat(sprintf("Dataframe: '%s'\n", data_name))
  
  # Iterate over each column by name
  for (col_name in colnames(data)) {
    # Check if the current column has any NA values
    na_rows <- sum(is.na(data[[col_name]]))
    has_na <- na_rows > 0
    
    # Print the column name, NA check result, and number of NA rows if any
    cat(sprintf("  - Column '%s': %s", col_name, ifelse(has_na, "TRUE", "FALSE")))
    
    # If NA values are present, also print the number of rows with NA
    if (has_na) {
      cat(sprintf(", NA rows: %d\n", na_rows))
    } else {
      cat("\n") # Just move to the next line if no NAs
    }
  }
  cat("\n") # Add an extra newline for better separation between dataframes
}



```

```{r}
# Converts string DATETIME into dttm and creates timestamp columns
wrangle_demand_data <- function(df) {
  if ("DATETIME" %in% names(df)) {
    df <- df %>%
      mutate(DATETIME = dmy_hm(`DATETIME`, tz='AUstralia/Brisbane'), # Converst DATETIME to dttm 
                          Year = year(DATETIME),        # Extract year
                          Month = month(DATETIME),      # Extract month
                          Day = day(DATETIME),          # Extract day
                          Hour = hour(DATETIME),        # Extract hour
                          Minute = minute(DATETIME)) %>%  # Extract minute
      mutate(TimeOfDay = Hour + Minute / 60,   # Time of the day (0 - 24)
             WeekOfMonth = factor(week(DATETIME) - week(floor_date(DATETIME, "month")) + 1, levels = c(1,2,3,4,5,6)),
             DayOfWeek = factor(wday(DATETIME, label=TRUE), levels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")))
  } else {
    warning("DATETIME column not found in the dataframe.")
  }
  return(df)
}
```


### Dmeand Data

```{r}
head(data_demand_nsw)
```



#### Demand data

```{r}
summary(data_demand_nsw)

check_for_na(data_demand_nsw)

df_demand_nsw <- wrangle_demand_data(data_demand_nsw)

head(df_demand_nsw)

```

```{r}

df_demand_nsw %>% 
  ggplot(aes(x=DATETIME, y=TOTALDEMAND)) +
  geom_line() +
  geom_vline(data = data.frame(x = seq(from = floor_date(min(df_demand_nsw$DATETIME), "year"),
                                       to = ceiling_date(max(df_demand_nsw$DATETIME), "year"),
                                       by = "1 year")), 
             aes(xintercept = as.numeric(x)), 
             linetype = "dotted", color = "red") +
  scale_x_datetime(
    date_breaks = '1 year',
    date_labels = '%Y'
  ) +
  theme_minimal()


```

```{r , message=FALSE}

df_demand_nsw_monthly <- df_demand_nsw %>% 
  group_by(Month, WeekOfMonth, TimeOfDay) %>% 
  summarise(MeanDemand = mean(TOTALDEMAND)) %>% 
  ungroup()

df_demand_nsw_monthly %>% 
  ggplot(aes(x = TimeOfDay, y = MeanDemand)) +
  geom_line(aes(color = WeekOfMonth)) +
  facet_wrap(~ Month, ncol = 3) + 
  scale_x_continuous(name = "Time of Day", breaks = seq(0, 24, by = 3), labels = function(x) sprintf("%02d:00", x)) +
  scale_y_continuous(name = "Mean Total Demand") +
  labs(title = "Weekly Pattern of Electricity Demand by Month") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        strip.text.x = element_text(size = 8))
```

```{r}

df_demand_nsw_seasonly <- df_demand_nsw %>% 
  mutate(Season = case_when(
    Month %in% c(12, 1, 2) ~ "Summer",
    Month %in% c(3, 4, 5) ~ "Autumn",
    Month %in% c(6, 7, 8) ~ "Winter",
    Month %in% c(9, 10, 11) ~ "Spring",
    TRUE ~ NA_character_))

df_demand_nsw_seasonly %>% 
  ggplot(aes(x = factor(Hour), y = TOTALDEMAND)) +
  geom_boxplot(aes(fill=Season)) +
  facet_wrap(~Season, scales = "free_y") + # Faceting by season
  labs(title = "Electricity Demand by Hour Across Seasons",
       x = "Hour of Day",
       y = "Total Demand") +
  scale_x_discrete(name = "Time of Day", breaks = seq(0, 24, by = 2)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1)) # Improve readability of x axis labels


```

```{r ,message=FALSE}

df_demand_nsw_weekly <- df_demand_nsw %>% 
  group_by(DayOfWeek, TimeOfDay) %>% 
  summarise(MeanDemand = mean(TOTALDEMAND, na.rm = TRUE))

df_demand_nsw_weekly %>% ggplot(aes(x = TimeOfDay, y = MeanDemand)) +
  geom_line(aes(color = DayOfWeek)) +
  scale_x_continuous(name = "Time of Day", breaks = seq(0, 24, by = 1), labels = function(x) sprintf("%02d:00", x)) +
  scale_y_continuous(name = "Mean Total Demand") +
  scale_color_brewer(palette = "Dark2") +
  labs(title = "Mean Total Demand by Time of Day and Day of Week") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```


```{r , message=FALSE}

df_demand_nsw_daily <- df_demand_nsw %>%
  group_by(Month, Day) %>% 
  summarise(Avg_daily_demand = mean(TOTALDEMAND)) %>% 
  mutate(DATE = make_date(year=2012, month=Month, day=Day),
         MonthName = month(DATE, label=TRUE))

df_demand_nsw_daily %>% 
  ggplot(aes(x = DATE, y = Avg_daily_demand)) +
  geom_line() + # Draw lines between points
  geom_point() + # Add points for each data point
  geom_smooth() +
  scale_x_date(date_breaks = "1 month", date_labels = "%b") + # Customize x-axis labels
  labs(x = "Month", y = "Avg demand", title = "Average daily demand") +
  theme_minimal() # Use a minimal theme


```

Some days are significantly lower demand, suspect to be holidays.

### Temperature data

```{r}
summary(data_temperature_nsw)
check_for_na(data_temperature_nsw)
df_temperature_nsw <- wrangle_demand_data(data_temperature_nsw)
head(df_temperature_nsw)

```


##### Align temperature data with demand data

Since the temperature dataset does not have a constant time frequency, we adjust an aceptable range of time to align with the 0 minutes and 30 minutes mark of demand data.

```{r}
df_temperature_nsw <- df_temperature_nsw %>%
  mutate(out_of_line = !(((Minute >= 55) | (Minute <= 5)) | ((Minute >= 25) & (Minute <= 35))))

# Count how many intervals are out of the expected line
sum_out_of_line <- sum(df_temperature_nsw$out_of_line, na.rm = TRUE)

# Print the count of out-of-line intervals
print(sum_out_of_line)

```
There are 16089 numbers of out of line data.



```{r}

df_aligned_temperature_nsw <- df_temperature_nsw %>%
  # Create a new column for the aligned timestamp
  mutate(DATETIME = DATETIME %>%
           # Round down or up to the nearest 30-minute mark
           floor_date(unit = "30 minutes") %>%
           # Adjust the rounding (55-05 -> 0, 25-35 -> 30)
           {if_else(minute(.) >= 55 | minute(.) <= 5, . - minutes(minute(.)), .)} %>%
           {if_else(minute(.) >= 25 & minute(.) <= 35, . + minutes(30 - minute(.)), .)}) %>%
  # Group by this new column
  group_by(DATETIME) %>% 
  # Calculate the mean temperature for each group
  summarise(TEMPERATURE = mean(TEMPERATURE))

```

```{r}
head(df_aligned_temperature_nsw)
```


```{r}


df_aligned_temperature_nsw %>% 
  ggplot(aes(x=DATETIME, y=TEMPERATURE)) +
  geom_line() +
  geom_vline(data = data.frame(x = seq(from = floor_date(min(df_temperature_nsw$DATETIME), "year"),
                                       to = ceiling_date(max(df_temperature_nsw$DATETIME), "year"),
                                       by = "1 year")), 
             aes(xintercept = as.numeric(x)), 
             linetype = "dotted", color = "red") +
  scale_x_datetime(
    date_breaks = '1 year',
    date_labels = '%Y'
  ) +
  theme_minimal()


```


## Forecast Data

```{r}
head(data_forecastdemand_nsw)

```

```{r}
df_forecastdemand_nsw_min <- data_forecastdemand_nsw %>%
  group_by(DATETIME) %>%
  filter(LASTCHANGED == min(LASTCHANGED)) 

df_forecastdemand_nsw_max <- data_forecastdemand_nsw %>%
  group_by(DATETIME) %>%
  filter(LASTCHANGED == max(LASTCHANGED)) 


```


## Solar Exposure

```{r}
head(solar_expo_nsw)
```


```{r}

df_solar_expo_nsw <- solar_expo_nsw %>% 
  mutate(
    Year = as.integer(Year), 
    Month = as.integer(Month), 
    Day = as.integer(Day),
    DATE = make_date(Year, Month, Day),
    Solar_exposure = `Daily global solar exposure (MJ/m*m)`
  ) %>% 
  filter(DATE >= as.Date("2010-01-01") & DATE <= as.Date("2021-03-17")) %>% 
  select(DATE, Solar_exposure)


```


```{r}
check_for_na(df_solar_expo_nsw)

```

1 NA value is found.



```{r}
df_solar_expo_nsw <- df_solar_expo_nsw %>% 
  filter(!is.na(Solar_exposure))

df_solar_expo_nsw %>%
  ggplot(aes(DATE, Solar_exposure)) +
  geom_line() +
  geom_vline(data = data.frame(Year = unique(year(df_solar_expo_nsw$DATE))), 
             aes(xintercept = as.Date(paste0(Year, "-01-01"))), 
             linetype = "solid", color = "red") +
  theme_minimal() +
  ggtitle("Solar Exposure Over Time in NSW") +
  xlab("Year") +
  ylab("Solar Exposure")

```

```{r}
df_solar_expo_nsw %>%
  mutate(Month = month(DATE, label = TRUE)) %>%
  ggplot(aes(x = Month, y = Solar_exposure)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Solar Exposure by Month", x = "Month", y = "Solar Exposure")
```

### Rainfall 

```{r}
head(rainfall_nsw)

```

```{r}

df_rainfall_nsw <- rainfall_nsw %>%
  mutate(
    Year = as.integer(Year), 
    Month = as.integer(Month), 
    Day = as.integer(Day),
    DATE = make_date(Year, Month, Day),
    Rainfall_amount = `Rainfall amount (millimetres)`
  ) %>% 
  filter(DATE >= as.Date("2010-01-01") & DATE <= as.Date("2021-03-17")) %>% 
  select(DATE, Rainfall_amount)
```


```{r}

check_for_na(df_rainfall_nsw)


```
93 NA values, convert to median 


```{r}
# Calculate the median of Rainfall_amount excluding NA values
median_rainfall <- median(df_rainfall_nsw$Rainfall_amount, na.rm = TRUE)

# Impute NA values in Rainfall_amount with the median
df_rainfall_nsw <- df_rainfall_nsw %>%
  mutate(Rainfall_amount = ifelse(is.na(Rainfall_amount), median_rainfall, Rainfall_amount))


```

```{r}
df_rainfall_nsw %>% 
  ggplot(aes(x=DATE, y=Rainfall_amount)) +
  geom_line() +
  theme_minimal()


```

##### GDP

```{r}
head(gdp)

```


```{r}

df_gdp <- gdp %>% 
  pivot_longer(cols=as.character(seq(1960,2022)),
                               names_to = 'Year',
                               values_to = 'GDP') %>% 
  mutate(Year = as.integer(Year)) %>% 
  select(Year, GDP) %>% 
  filter(Year >= 2010)

```

```{r}
head(df_gdp)

```


## Bivariate Analysis

##### Holidays 

```{r}

df_demand_nsw <- df_demand_nsw %>% 
  mutate(Date = date(DATETIME)) %>% 
  mutate(
    IsHoliday = case_when(
      date(DATETIME) %in% holidays_nsw$Date ~ 'Holiday',
      TRUE ~ 'Non-Holiday'
    ))

# Boxplot to compare electricity demand on holidays vs non-holidays
df_demand_nsw %>% 
  ggplot(aes(x = IsHoliday, y = TOTALDEMAND)) +
  geom_boxplot(aes(fill=IsHoliday)) +
  labs(title = "Electricity Demand on Holidays vs. Non-Holidays",
       x = "", y = "Total Demand") +
  theme_minimal()


```




##### Demand vs Temperature
```{r}
# Inner join to get only matched data
df_nsw <- inner_join(df_demand_nsw, df_aligned_temperature_nsw, by = "DATETIME")

```


```{r}


# Outer join to find unmatched data
unmatched_demand <- anti_join(df_demand_nsw, df_aligned_temperature_nsw, by = "DATETIME")
unmatched_temperature <- anti_join(df_aligned_temperature_nsw, df_demand_nsw, by = "DATETIME")

# Combine the unmatched data into one tibble
excluded_data <- bind_rows(unmatched_demand, unmatched_temperature)

# Optionally, add a column to indicate the source of each row in the excluded data
excluded_data <- excluded_data %>%
  mutate(Source = ifelse(is.na(TEMPERATURE), "Demand Only", "Temperature Only"))

nrow(excluded_data)

unique(excluded_data$Source)
```
Correlation testing
```{r}

cor.test(df_nsw$TEMPERATURE, df_nsw$TOTALDEMAND, use = "pearson")

```
p-value is small reject null-hypothesis.


```{r}


df_nsw %>% 
  ggplot(aes(x=TEMPERATURE, y=TOTALDEMAND)) +
  geom_point() +
  labs(title = "Relationship between Temperature and Electricity Demand",
       x = "Temperature",
       y = "Electricity Demand")




```



```{r}
# Converts to daily data to match solar exposure and rainfall 
df_nsw_daily <- df_nsw %>% 
  mutate(DATE = as.Date(DATETIME)) %>% 
  group_by(DATE) %>% 
  summarise(Daily_Mean_Temperature = mean(TEMPERATURE, na.rm = TRUE),
            Daily_Max_Temperature = max(TEMPERATURE, na.rm = TRUE),
            Daily_Min_Temperature = min(TEMPERATURE, na.rm = TRUE),
            Daily_Mean_Demand = mean(TOTALDEMAND, na.rm = TRUE),
            Daily_Max_Demand = max(TOTALDEMAND, na.rm = TRUE),
            Daily_Min_Demand = min(TOTALDEMAND, na.rm = TRUE)) %>% 
    ungroup() %>% 
    mutate(WeekOfMonth = factor(week(DATE) - week(floor_date(DATE, "month")) + 1, levels = c(1,2,3,4,5,6)),
           DayOfWeek = factor(wday(DATE, label=TRUE), levels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")))
  
df_nsw_daily <- inner_join(df_nsw_daily, df_solar_expo_nsw, by = 'DATE')
```



```{r}

# Transform the data from wide to long format
df_nsw_daily_long <- df_nsw_daily %>%
  pivot_longer(cols = c("Daily_Mean_Demand", "Daily_Max_Demand", "Daily_Min_Demand"),
               names_to = "Demand_Type",
               values_to = "Demand") %>% 
    pivot_longer(cols = c("Daily_Mean_Temperature", "Daily_Max_Temperature", "Daily_Min_Temperature"),
               names_to = "Temperature_Type",
               values_to = "Temperature")
```



##### Daily Demand vs Solar Exposure
```{r}
# Plotting with faceting
ggplot(df_nsw_daily_long, aes(x = Solar_exposure, y = Demand)) +
  geom_point() +
  geom_smooth(se = FALSE, method = "loess") + 
  ggtitle("Solar Exposure vs Daily Demand in NSW") +
  xlab("Solar Exposure") +
  ylab("Demand") +
  facet_wrap(~Demand_Type, scales = "free_y") + # Create a separate panel for each Demand_Type
  theme_minimal()


```

##### Daily Temperature vs SOlar Exposure

```{r}

# Plotting with faceting
ggplot(df_nsw_daily_long, aes(x = Solar_exposure, y = Temperature)) +
  geom_point() +
  geom_smooth(se = FALSE, method = "loess") + 
  ggtitle("Solar Exposure vs Temperature in NSW") +
  xlab("Solar Exposure") +
  ylab("Temperature") +
  facet_wrap(~Temperature_Type, scales = "free_y") + 
  theme_minimal()


```




```{r}
df_nsw_daily <- inner_join(df_nsw_daily, df_rainfall_nsw)
df_nsw_daily_long <- inner_join(df_nsw_daily_long, df_rainfall_nsw)



# Plotting with faceting
ggplot(df_nsw_daily_long, aes(x = Rainfall_amount, y = Demand)) +
  geom_point() +
  geom_smooth(se = FALSE, method = "loess") + 
  ggtitle("Daily Demand vs Rainfall Amount in NSW") +
  xlab("Rainfall Amount") +
  ylab("Electricity Demand") +
  facet_wrap(~Demand_Type, scales = "free_y") + # Create a separate panel for each Demand_Type
  theme_minimal()



```


```{r}

cor.test(df_nsw_daily$Daily_Mean_Demand, df_nsw_daily$Solar_exposure, use="pearson")
cor.test(df_nsw_daily$Daily_Max_Demand, df_nsw_daily$Solar_exposure, use="pearson")
cor.test(df_nsw_daily$Daily_Min_Demand, df_nsw_daily$Solar_exposure, use="pearson")
cor.test(df_nsw_daily$Daily_Mean_Temperature, df_nsw_daily$Solar_exposure, use="pearson")
cor.test(df_nsw_daily$Daily_Max_Temperature, df_nsw_daily$Solar_exposure, use="pearson")
cor.test(df_nsw_daily$Daily_Min_Temperature, df_nsw_daily$Solar_exposure, use="pearson")

```



##### GDP vs Demand

```{r}
# Converts to yearly data to match GDP dataset
df_demand_nsw_yearly = df_demand_nsw %>% 
  mutate(Year = year(DATETIME)) %>% 
  group_by(Year) %>% 
  summarise(Avg_yearly_demand = mean(TOTALDEMAND))

df_nsw_yearly <- inner_join(df_demand_nsw_yearly, df_gdp, by = 'Year')
```

```{r}

df_nsw_yearly %>% 
  ggplot(aes(GDP, Avg_yearly_demand)) +
  geom_line() +
  geom_smooth()


```

```{r}

cor.test(df_nsw_yearly$GDP, df_nsw_yearly$Avg_yearly_demand, use="pearson")

```
Not statistically significant. Accept null hypothesis, no correlation.



#### Forecast Data

```{r}
# Uses the latest last change date
df_forecastdemand_nsw <- data_forecastdemand_nsw %>% 
  group_by(DATETIME) %>%
  filter(LASTCHANGED == max(LASTCHANGED)) %>%
  ungroup() 

```

```{r}
head(df_forecastdemand_nsw)


```

```{r}


df_max <- inner_join(df_forecastdemand_nsw_max, df_demand_nsw, by='DATETIME')
df_min <- inner_join(df_forecastdemand_nsw_min, df_demand_nsw, by='DATETIME')
```

```{r}


df_max <- df_max %>% 
  mutate(DATE = as.Date(DATETIME))
df_min <- df_min %>% 
  mutate(DATE = as.Date(DATETIME))


dates_2017_max <- unique(df_max %>% 
  filter(Year == 2017))
dates_2017_min <- unique(df_min %>% 
  filter(Year == 2017))

set.seed(123)

# Sample 6 random dates
sampled_dates <- sample(dates_2017_min$DATE, 6)



sampled_df_max <- df_max %>%
  filter(DATE %in% sampled_dates)

sampled_df_min <- df_min %>%
  filter(DATE %in% sampled_dates)

```


```{r}
ggplot(sampled_df_max, aes(x = DATETIME)) + 
  geom_line(aes(y = FORECASTDEMAND, color = "Forecast Demand"), linetype = "dashed", size = 1.2) + 
  geom_line(aes(y = TOTALDEMAND, color = "Total Demand"), size = 1.2) +
  geom_point(aes(y = FORECASTDEMAND, color = "Forecast Demand"), size = 2, alpha = 0.5) + 
  geom_point(aes(y = TOTALDEMAND, color = "Total Demand"), size = 2, alpha = 0.5) +
  labs(title = "Forecast vs Total Demand on Random Days in 2017",
       x = "Hour of the Day",
       y = "Demand",
       color = "Legend") +
  scale_color_manual(values = c("Forecast Demand" = "red", "Total Demand" = "blue")) +
  scale_x_datetime(date_labels = "%H", date_breaks = "2 hours") +
  theme_minimal() +
  theme(legend.title = element_blank(),
        legend.position = "bottom",
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~DATE, ncol = 3, scales = "free_x")


```

```{r}
ggplot(sampled_df_min, aes(x = DATETIME)) + 
  geom_line(aes(y = FORECASTDEMAND, color = "Forecast Demand"), linetype = "dashed", size = 1.2) + 
  geom_line(aes(y = TOTALDEMAND, color = "Total Demand"), size = 1.2) +
  geom_point(aes(y = FORECASTDEMAND, color = "Forecast Demand"), size = 2, alpha = 0.5) + 
  geom_point(aes(y = TOTALDEMAND, color = "Total Demand"), size = 2, alpha = 0.5) +
  labs(title = "Forecast vs Total Demand on Random Days in 2017",
       x = "Hour of the Day",
       y = "Demand",
       color = "Legend") +
  scale_color_manual(values = c("Forecast Demand" = "red", "Total Demand" = "blue")) +
  scale_x_datetime(date_labels = "%H", date_breaks = "2 hours") +
  theme_minimal() +
  theme(legend.title = element_blank(),
        legend.position = "bottom",
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~DATE, ncol = 3, scales = "free_x")


```

##### Accuracy

```{r}

df_nsw <- inner_join(df_nsw, df_forecastdemand_nsw_max, by='DATETIME')

```

```{r}
#df_nsw %>%
  #select(DATETIME, FORECASTDEMAND.x, TOTALDEMAND) %>% 
  #summarise(MAE = mean(abs(FORECASTDEMAND.x - TOTALDEMAND), na.rm = TRUE))

# Calculate RMSE
#rmse_value <- sqrt(mean((df_nsw$FORECASTDEMAND.x - df_nsw$TOTALDEMAND)^2, na.rm = TRUE))
#print(rmse_value)

# Calculate MAPE
#df_nsw %>%
  #filter(TOTALDEMAND != 0) %>% # Avoid division by zero
  #summarise(MAPE = mean(abs((FORECASTDEMAND.x - TOTALDEMAND) / TOTALDEMAND), na.rm = TRUE) * 100)


```


### Correlation heatmap

##### Demand vs Time

```{r}

cor_df <- df_demand_nsw %>% 
  select(-DATETIME, -REGIONID, -Date) %>% 
  mutate(IsHoliday = case_when(IsHoliday == 'Holiday' ~ 0, TRUE ~ 1)) %>% 
  mutate(across(everything(), as.numeric)) %>% 
  select(-Day,-Hour)


cor_matrix <- cor(cor_df)


corrplot(cor_matrix, method = "color", type = "full", order = "hclust",
         tl.col = "black", tl.srt = 45, 
         addCoef.col = "black") 


```


##### Daily data

```{r}

cor_df2 <- df_nsw_daily %>% 
  select(-DATE, -WeekOfMonth, -DayOfWeek) 

cor_matrix2 <- cor(cor_df2)


corrplot(cor_matrix2, method = "color", type = "full", order = "hclust",
         tl.col = "black", tl.srt = 45, 
         addCoef.col = "black") 

```

##### Autocorrelation

```{r}
dfts_demand_nsw <- as_tsibble(df_demand_nsw %>% select(DATETIME, TOTALDEMAND), index=DATETIME)
```



```{r, message=FALSE}

dfts_demand_nsw %>% 
  autoplot(TOTALDEMAND %>%  difference(1)) 

dfts_demand_nsw %>% features(TOTALDEMAND, unitroot_kpss)

dfts_demand_nsw %>%
  mutate(diff_TOTALDEMAND = difference(TOTALDEMAND)) %>% 
  features(diff_TOTALDEMAND, unitroot_kpss)

dfts_demand_nsw %>% features(TOTALDEMAND, unitroot_ndiffs)
```


```{r}
# 0.01 Non-stationary
dfts_demand_nsw %>% 
  features(TOTALDEMAND, unitroot_kpss) 


# 0.1 Stationary after first order differencing
dfts_demand_nsw %>% 
  mutate(diff_TOTALDEMAND = difference(TOTALDEMAND, 48)) %>% 
  features(diff_TOTALDEMAND, unitroot_kpss) 


```

KPSS suggests after differencing the model is stationary. (Both non-seasonal and seasonal)



```{r}

library(gridExtra)

# Load or prepare your data frame `dfts_demand_nsw` with a column `TOTALDEMAND`
# dfts_demand_nsw <- your_data_frame

# Calculate ACF for original data
plot1 <- dfts_demand_nsw %>% 
  ACF(TOTALDEMAND, lag_max = 336*2) %>% 
  autoplot() + 
  ggtitle("ACF of TOTALDEMAND") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_continuous(breaks = seq(0, 336*2, by = 96))  # Set x-axis breaks

# Calculate PACF for original data
plot2 <- dfts_demand_nsw %>% 
  PACF(TOTALDEMAND, lag_max = 336) %>% 
  autoplot() + 
  ggtitle("PACF of TOTALDEMAND") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_continuous(breaks = seq(0, 336, by = 96))  # Set x-axis breaks

# Calculate ACF for differenced data
plot3 <- dfts_demand_nsw %>% 
  mutate(diff_TOTALDEMAND = difference(TOTALDEMAND, 48)) %>% 
  ACF(diff_TOTALDEMAND, lag_max = 336*2) %>% 
  autoplot() + 
  ggtitle("ACF of differenced TOTALDEMAND") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_continuous(breaks = seq(0, 336*2, by = 96))  # Set x-axis breaks

# Calculate PACF for differenced data
plot4 <- dfts_demand_nsw %>% 
  mutate(diff_TOTALDEMAND = difference(TOTALDEMAND, 48)) %>% 
  PACF(diff_TOTALDEMAND, lag_max = 336) %>% 
  autoplot() + 
  ggtitle("PACF of differenced TOTALDEMAND") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_continuous(breaks = seq(0, 336, by = 96))  # Set x-axis breaks

# Combine all plots into one with a common title
grid.arrange(plot1, plot2, plot3, plot4, ncol = 2)

```

ACF and PACF after first order differencing. 

PACF significant lag at lag 1, 49, 97, ... Suggest at least AR(1) and SAR(1)

Seasonal: Significant at multiple lags, 96, 192, 240, 360 suggests at least MA(2) and SMA(2)





#### STL decomposition 
##### Extracts the trend and seasonal component of the 

```{r}
electricity_ts <- df_demand_nsw %>%
  select(DATETIME, TOTALDEMAND) %>% 
  as_tsibble(index = DATETIME) 

```


```{r}

decomposed <- electricity_ts %>%
  model(STL = STL(TOTALDEMAND ~ season(window = "periodic"),
                  robust = TRUE)) %>%
  components()

# Extracting seasonal components
seasonal_components <- decomposed %>%
  select(trend, season_hour , season_day, season_week, season_year ) 


```




```{r}

lm_model <- decomposed %>%
  model(lm = TSLM(TOTALDEMAND  ~ trend + season_hour + season_day + season_week + season_year))

report(lm_model)

```

```{r}
lm <-
  lm(TOTALDEMAND ~ TimeOfDay + as.numeric(WeekOfMonth)+ as.numeric(DayOfWeek), data=df_demand_nsw)

summary(lm)
```

















## Re-cleaning Data
```{r}
df_demand_nsw <- df_demand_nsw %>% 
  mutate(Date = date(DATETIME)) %>% 
  mutate(
    IsHoliday = case_when(
      date(DATETIME) %in% holidays_nsw$Date ~ TRUE,
      TRUE ~ FALSE
    ))


```

```{r}

df_aligned_temperature_nsw <- df_temperature_nsw %>%
  # Create a new column for the aligned timestamp
  mutate(DATETIME = DATETIME %>%
           # Round down or up to the nearest 30-minute mark
           floor_date(unit = "30 minutes") %>%
           # Adjust the rounding (45-14 -> 0, 15-44 -> 30)
           {if_else(minute(.) >= 45 | minute(.) <= 14, . - minutes(minute(.)), .)} %>%
           {if_else(minute(.) >= 15 & minute(.) <= 44, . + minutes(30 - minute(.)), .)}) %>%
  # Group by this new column
  group_by(DATETIME) %>% 
  # Calculate the mean temperature for each group
  summarise(TEMPERATURE = mean(TEMPERATURE))

```


```{r}

df_lj <- left_join(df_demand_nsw, df_aligned_temperature_nsw, by='DATETIME')

```

```{r}
# Fill NA values
df_lj$TEMPERATURE <- na.spline(df_lj$TEMPERATURE)
check_for_na(df_lj)

```

```{r}

df_nsw <- left_join(df_lj, df_forecastdemand_nsw_max, by='DATETIME')

```



```{r}
# Uses the latest last change date
df_forecastdemand_nsw <- data_forecastdemand_nsw %>% 
  mutate(DATETIME = as_datetime(DATETIME, tz='AUstralia/Brisbane')) %>% 
  group_by(DATETIME) %>%
  filter(LASTCHANGED == max(LASTCHANGED)) %>%
  summarize(FORECASTDEMAND = mean(FORECASTDEMAND), .groups = 'drop') %>% 
  ungroup() 



```

```{r}

df_nsw$FORECASTDEMAND <- na.spline(df_nsw$FORECASTDEMAND)
check_for_na(df_nsw)
```
```{r}


df_nsw <- left_join(df_lj, df_forecastdemand_nsw, by='DATETIME')

```

```{r}
df <- df_nsw %>% 
  select(DATETIME, TOTALDEMAND, TEMPERATURE, FORECASTDEMAND, IsHoliday) %>% 
  mutate(
    TEMPERATURE_SQ = TEMPERATURE^2
  ) %>%
  as_tsibble(index = DATETIME)


```


```{r}
start_date <- ymd("2015-06-01")
end_date <- ymd("2018-06-30")

dfts_nsw <- df %>% 
  select(DATETIME, TOTALDEMAND, TEMPERATURE, FORECASTDEMAND,TEMPERATURE_SQ) %>% 
  as_tsibble(index = DATETIME) %>% 
  filter(DATETIME >= start_date & DATETIME <= end_date)


```

# Modelling 

```{r}
n <- nrow(dfts_nsw)

train_set <- dfts_nsw[1:(n-48), ]
test_set <- dfts_nsw[(n-47):n, ]
```

```{r}

train_set %>% 
  gg_tsdisplay(
    difference(difference(TOTALDEMAND, lag = 48), lag = 1), 
    plot_type = 'partial', 
    lag_max = 48*7
  ) +
  labs(title = 'Double Differenced', y = '')

```


```{r}
fit <- train_set %>% 
  model(
    arima_fr = ARIMA(TOTALDEMAND ~ pdq(d=1) + PDQ(0,0,0) +
                        fourier(period = 48, K = 3) +     # Daily 
                        fourier(period = 48*7, K = 3)),   # Weekly
    sarima_212210 = ARIMA(TOTALDEMAND ~ pdq(2,1,2) + PDQ(2,1,0, period=48)),
    
    sarima_212210_fr = ARIMA(TOTALDEMAND ~ pdq(2,1,2) + PDQ(2,1,0, period=48) +
                        fourier(period = 48*7, K = 3)), # Weekly
    
    sarima_auto_temp = ARIMA(TOTALDEMAND ~ pdq(d=1) + PDQ(D=1, period=48) + TEMPERATURE + TEMPERATURE_SQ),
    
    sarima_212210_temp_fr = ARIMA(TOTALDEMAND ~ pdq(2,1,2) + PDQ(2,1,0, period=48) + 
                                TEMPERATURE + TEMPERATURE_SQ +
                                fourier(period = 48*7, K = 3))    # Weekly
  )

```

```{r}
## Show all models fitted

fit %>% pivot_longer(everything(), names_to = "Model name", values_to = "Orders")

## Order by smallest BIC
glance(fit) %>% arrange(BIC) %>% select(.model:BIC)
```

```{r}
# Forecasting
forecasts <- fit %>%
  forecast(test_set)

```

```{r}
forecasts %>% accuracy(df) %>% arrange(RMSE, 'desc')

```


```{r}


forecasts %>% autoplot(tail(dfts_nsw, 96), level=NULL, size=1.2, alpha=0.9)

```



```{r}
# comparing adding fourier and without fourier

selected_models_forecasts <- forecasts %>%
  filter(.model %in% c('sarima_212210', 'sarima_212210_fr'))

selected_models_forecasts %>% autoplot(tail(dfts_nsw, 96), level=NULL, size=1.2, alpha=0.9)
```

Selection of the final model:

SARIMA_212210_fr



```{r}

initial_train_end <- nrow(dfts_nsw) - 48 * 5  # Training data ends 10 days before the dataset's end
forecast_horizon <- 48  # Forecasting one day ahead (48 half-hour intervals)
forecasts_list <- vector("list", 5)  # Store each forecast

# Loop through each forecast iteration
for (i in 1:5) {
  # Define the training data up to the current endpoint
  train_data <- dfts_nsw[1:(initial_train_end + (i - 1) * forecast_horizon), ]
  
  cat(sprintf("Fit %d starting: %s\n", i, Sys.time()))
  # Fit the ARIMA model with Fourier terms
  fit_arima <- train_data %>%
    model(sarima_212210_temp_fr = ARIMA(TOTALDEMAND ~ pdq(2, 1, 2) + PDQ(2, 1, 0, period = 48) +
                                        TEMPERATURE + TEMPERATURE_SQ +
                                        fourier(K = 3, period = 48*7)))
  
  cat(sprintf("Fit %d done: %s\n", i, Sys.time()))
  # Define new_data for forecasting: next 48 observations beyond the training set
  test_set <- dfts_nsw[(initial_train_end + (i - 1) * forecast_horizon + 1):(initial_train_end + i * forecast_horizon), ]

  # Forecast using the model
  forecasts <- forecast(fit_arima, new_data = test_set)
  forecasts_list[[i]] <- forecasts
}

# Combine forecasts into a single tsibble for plotting or evaluation
combined_forecasts <- bind_rows(forecasts_list) %>% as_tibble()


```



```{r}
# Extract forecast means
forecast_means <- combined_forecasts %>%
  select(.mean) %>%
  pull()

# Create a time series object from the forecasted means
pred_arima_ts <- ts(forecast_means, start = c(ceiling((initial_train_end + 1) / 48), (initial_train_end + 1) %% 48), frequency = 48)

# Actual data corresponding to the forecast periods
actual_indices <- (initial_train_end + 1):(initial_train_end + 10 * forecast_horizon)
actual_data <- dfts_nsw$TOTALDEMAND[actual_indices]

# Calculate accuracy metrics
accuracy_metrics <- accuracy(pred_arima_ts, actual_data)
print(accuracy_metrics)


```

```{r}

# Create a data frame for plotting
plot_data <- data.frame(
  Time = seq_along(actual_data),
  Actual = actual_data,
  Forecast = as.numeric(pred_arima_ts)
)

# Creating the plot
p <- ggplot(plot_data, aes(x = Time)) +
  geom_line(aes(y = Actual, colour = "Actual"), size = 1) +
  geom_line(aes(y = Forecast, colour = "Forecast"), size = 1, linetype = "dashed") +
  scale_color_manual(values = c("Actual" = "blue", "Forecast" = "red")) +
  labs(title = "ARIMA Forecast vs Actual Data",
       x = "Time (in 48 half-hour intervals per day)",
       y = "Total Demand",
       colour = "Legend") +
  theme_minimal()

# Display the plot
print(p)




```







```{r}

# Set up parallel processing
num_cores <- detectCores() - 4
cl <- makeCluster(num_cores)
registerDoParallel(cl)
plan(multisession, workers = num_cores)
```


```{r}
stopCluster(cl)
registerDoSEQ()  # Switch back to sequential processing
plan(sequential)
```
























############# NOT RELAVENT (KEEP JUST IN CASE)

```
  combined_data <- df %>%
    left_join(forecasts, by = "DATETIME") %>% 
    filter(DATETIME >= ymd("2018-06-20") & DATETIME <= ymd("2018-06-30"))

  ggplot(combined_data) +
    geom_line(aes(x = DATETIME, y = TOTALDEMAND.x, color = "Actual Demand"), size = 1.5) +
    geom_line(aes(x = DATETIME, y = .mean, color = "Forecast Demand"), size = 1.5) +
    scale_color_manual(values = c("Actual Demand" = "black",
                                  "Forecast Demand" = "red")) +
    labs(title = "Total Demand Forecast vs Actual",
         x = "Time",
         y = "Total Demand",
         color = "Legend") +
    theme_minimal()
```




```{r}
#evaluate_forecast(df, forecasts_fr)
accuracy(forecasts_fr, df)
```

```{r}

e

```

## Normal SAMIRA
```{r}

fit <- train_set %>% 
  model(
    arima = ARIMA(TOTALDEMAND ~ pdq(d=1) + PDQ(D=1, period=48))
  )

```


```{r}

forecasts <- fit %>% 
  forecast(h=48*7)

```

```{r}
accuracy(forecasts, df)

```

```{R}
  combined_data <- df %>%
    left_join(forecasts, by = "DATETIME") %>% 
    filter(DATETIME >= ymd("2018-06-20") & DATETIME <= ymd("2018-06-30"))

  ggplot(combined_data) +
    geom_line(aes(x = DATETIME, y = TOTALDEMAND.x, color = "Actual Demand"), size = 1.5) +
    geom_line(aes(x = DATETIME, y = .mean, color = "Forecast Demand"), size = 1.5) +
    scale_color_manual(values = c("Actual Demand" = "black",
                                  "Forecast Demand" = "red")) +
    labs(title = "Total Demand Forecast vs Actual",
         x = "Time",
         y = "Total Demand",
         color = "Legend") +
    theme_minimal()
```


The auto arima gets  ARIMA(2,1,2)(2,1,0)[48]

# Try adding temperature^2
```{r}

  
fit_temp <- train_set %>% 
  model(
    arima_temp = ARIMA(TOTALDEMAND ~ pdq(2,1,2) + PDQ(2,1,0, period=48) +
                         TEMPERATURE_SQ)
  )


```


```{r}

forecasts_temp <- fit_temp %>% 
  forecast(h=48, test_set)

```

```{r}
accuracy(forecasts_temp, df)

```




```{r}


evaluate_forecast <- function(actual_data, forecast_data, start_date, end_date, benchmark) {
  
  # Merge actual and forecast data
  combined_data <- actual_data %>%
    left_join(forecast_data, by = "DATETIME") %>% 
    filter(DATETIME >= start_date & DATETIME <= end_date)
  
  # Calculate accuracy metrics
  accuracy_metrics <- accuracy(forecast_data, actual_data)
  
  # Print accuracy metrics
  print(accuracy_metrics)
  
  # Create a plot to compare actual vs forecast
  if (benchmark) {
    p <- ggplot(combined_data) +
      geom_line(aes(x = DATETIME, y = TOTALDEMAND.x, color = "Actual Demand"), size = 1.5) +
      geom_line(aes(x = DATETIME, y = .mean, color = "Forecast Demand"), size = 1.5) +
      geom_line(aes(x = DATETIME, y = FORECASTDEMAND.x, color = "Benchmark"), size = 1.5) +
      scale_color_manual(values = c("Actual Demand" = "black", "Forecast Demand" = "red", "Benchmark" = "blue")) +
      labs(title = "Total Demand Forecast vs Actual",
           x = "Time",
           y = "Total Demand",
           color = "Legend") +
      theme_minimal()
  } else {
    p <- ggplot(combined_data) +
      geom_line(aes(x = DATETIME, y = TOTALDEMAND.x, color = "Actual Demand"), size = 1.5) +
      geom_line(aes(x = DATETIME, y = .mean, color = "Forecast Demand"), size = 1.5) +
      scale_color_manual(values = c("Actual Demand" = "black", "Forecast Demand" = "red")) +
      labs(title = "Total Demand Forecast vs Actual",
           x = "Time",
           y = "Total Demand",
           color = "Legend") +
      theme_minimal()
  }
  
  # Print the plot
  print(p)
}

```

```{r}
evaluate_forecast(df,forecasts_temp,ymd("2018-06-28"), ymd("2018-06-30"),FALSE)

```

# Adding IsHoliday and IsWeekday

```{r}
fit_temp_holiday <- train_set %>% 
  model(
    arima_temp = ARIMA(TOTALDEMAND ~ pdq(2,1,2) + PDQ(2,1,0, period=48) +
                         TEMPERATURE + IsHoliday + IsWeekday)
  )



```

```{r}

forecasts_temp_holiday <- fit_temp %>% 
  forecast(test_set)

```

```{r}

accuracy(forecasts_temp_holiday, df)
```

```{r}

evaluate_forecast(df,forecasts_temp_holiday,ymd("2018-06-28"), ymd("2018-06-30"),FALSE)

```

```{r}


arima_fit <- train_set %>% 
  model(
    sarima_111212 = ARIMA(TOTALDEMAND ~ pdq(1,1,1) + PDQ(2,1,2, period=48))
  )

```


```
library(dplyr)
library(tidyr)
library(purrr)

# Function to expand daily data frames
expand_daily_data <- function(df, date_column_name, variables) {
  df %>%
    mutate(DATETIME = map(DATE, ~ seq(as.POSIXct(paste(.x, "00:00:00")),
                                      as.POSIXct(paste(.x, "23:30:00")),
                                      by = "30 min"))) %>%
    unnest(DATETIME) %>%
    select(DATETIME, all_of(variables))  # Preserve other relevant variables
}

# Expand df_solar_expo_nsw
df_solar_expo_expanded <- expand_daily_data(df_solar_expo_nsw, "DATE", c("Solar_exposure"))

# Expand df_rainfall_nsw
df_rainfall_expanded <- expand_daily_data(df_rainfall_nsw, "DATE", c("Rainfall_amount"))

```

```

df_nsw <- df_nsw %>%
  left_join(df_solar_expo_expanded, by = "DATETIME") %>%
  left_join(df_rainfall_expanded, by = "DATETIME")

```

