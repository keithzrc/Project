---
Title: "Assessment 3: Group Project"
Project Name: "Enhancing Electricity Demand Forecasting in New South Wales"
Team: "Team Echo"
Session: "Hexa 2, 2024"
Coursecode: "ZZSC9020"
authors:
  - "Chuang, Keith – z5449930"
  - "Gandhi, Rupesh – z5368767"
  
Date: "20 April 2024"

Acknowledgements: 
  - "Sonit Singh - Project Consultant"
  - "Wei Tan - Project Consultant"
  
output: 
  bookdown::html_document2:
    fig_caption: yes
    toc: yes
    number_sections: yes
    toc_float: true
    toc_depth: 3 
---
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r libraries, warning=FALSE, message=FALSE}
# Import libraries 
library(ggplot2)
library(tidyverse)
library(lubridate)
library(fpp3)
library(fable)
library(fabletools)
library(feasts)
library(corrplot)
library(forecast)
library(tsibble)
library(zoo)
library(future)
library(bookdown)

```

# Introduction

This document provides a detailed analysis of electricity demand forecasting in New South Wales (NSW), exploring advanced statistical techniques and modeling approaches to enhance forecast accuracy. Utilizing datasets from January 2010 to March 2021, which include half-hourly electricity demand and temperature data, the report focuses on the development and refinement of forecasting models. We systematically apply Autoregressive Integrated Moving Average (ARIMA) models with daily and weekly Fourier terms, as well as Seasonal ARIMA (SARIMA) models with weekly Fourier terms, to identify the most effective approach for capturing complex seasonal and daily patterns in electricity demand.

# Data Preperation

## Data loading 
### Shared Data
```{r, message=FALSE}
# Shared data for the project
data_temperature_nsw <- read_csv(unzip("../data/NSW/temperature_nsw.csv.zip", files = "temperature_nsw.csv", exdir = tempdir())[1])
data_demand_nsw <- read_csv(unzip("../data/NSW/totaldemand_nsw.csv.zip", files = "totaldemand_nsw.csv", exdir = tempdir())[1])
```

```{r ,message=FALSE}
#data extraction from Github
data_forecastdemand_nsw_part1 <- read_csv(unzip("../data/NSW/forecastdemand/forecastdemand_part1.csv.zip", files = "forecastdemand_part1.csv", exdir = tempdir())[1])
data_forecastdemand_nsw_part2 <- read_csv(unzip("../data/NSW/forecastdemand/forecastdemand_part2.csv.zip", files = "forecastdemand_part2.csv", exdir = tempdir())[1])
data_forecastdemand_nsw_part3 <- read_csv(unzip("../data/NSW/forecastdemand/forecastdemand_part3.csv.zip", files = "forecastdemand_part3.csv", exdir = tempdir())[1])
data_forecastdemand_nsw_part4 <- read_csv(unzip("../data/NSW/forecastdemand/forecastdemand_part4.csv.zip", files = "forecastdemand_part4.csv", exdir = tempdir())[1])
data_forecastdemand_nsw_part5 <- read_csv(unzip("../data/NSW/forecastdemand/forecastdemand_part5.csv.zip", files = "forecastdemand_part5.csv", exdir = tempdir())[1])
```

```{r}
# Combine all forecastdemand into 1 df
data_forecastdemand_nsw <- bind_rows(data_forecastdemand_nsw_part1, data_forecastdemand_nsw_part2, data_forecastdemand_nsw_part3, data_forecastdemand_nsw_part4,data_forecastdemand_nsw_part5) 
# Clear memory
rm(data_forecastdemand_nsw_part1,data_forecastdemand_nsw_part2, data_forecastdemand_nsw_part3, data_forecastdemand_nsw_part4, data_forecastdemand_nsw_part5)
```


### Sourced Data

```{r, message=FALSE}
## Reference: https://www.michaelplazzer.com/datasets/australian-public-holiday-data/

holidays <- read_csv('../data/Aus_public_hols_2009-2022-1.csv')
holidays_nsw <- holidays %>% 
  filter(State == 'NSW')

#Further analysis and integration can be accomplished on the below variables, including solar, rainfall and population GDP. In our analysis we have not included this as part of our analysis, due to small team size

## Reference: http://www.bom.gov.au/climate/data/index.shtml
solar_expo_nsw <- read_csv("../data/daily_solar_exposure_bankstown_airport/IDCJAC0016_066137_1800_Data.csv")
## Reference: http://www.bom.gov.au/climate/data/index.shtml
rainfall_nsw <- read_csv("../data/daily_rainfall_bankstown_airport/IDCJAC0009_066137_1800_Data.csv")
```


## Initial Data cleaning

``` {r}
# Identification and Removal of Null Values: An anomaly was detected in Data, where a singular instance of a missing value (NA) was identified. Given the potential impact of missing data on the accuracy of our analysis, this NA value was promptly removed to maintain data integrity.

check_for_na <- function(data) {
  # Get the name of the dataframe
  data_name <- deparse(substitute(data))
  
  # Print the name of the dataframe
  cat(sprintf("Dataframe: '%s'\n", data_name))
  
  # Iterate over each column by name
  for (col_name in colnames(data)) {
    # Check if the current column has any NA values
    na_rows <- sum(is.na(data[[col_name]]))
    has_na <- na_rows > 0
    
    # Print the column name, NA check result, and number of NA rows if any
    cat(sprintf("  - Column '%s': %s", col_name, ifelse(has_na, "TRUE", "FALSE")))
    
    # If NA values are present, also print the number of rows with NA
    if (has_na) {
      cat(sprintf(", NA rows: %d\n", na_rows))
    } else {
      cat("\n") # Just move to the next line if no NAs
    }
  }
  cat("\n") # Add an extra newline for better separation between dataframes
}
```


```{r}

check_for_na(data_demand_nsw)
check_for_na(data_temperature_nsw)
check_for_na(data_forecastdemand_nsw)

```

# Data Exploration

## Electricity Demand

```{r}
# Converts string DATETIME into dttm and creates timestamp columns
wrangle_demand_data <- function(df) {
  if ("DATETIME" %in% names(df)) {
    df <- df %>%
      mutate(DATETIME = dmy_hm(`DATETIME`, tz='AUstralia/Brisbane'), # Converst DATETIME to dttm 
                          Year = year(DATETIME),        # Extract year
                          Month = month(DATETIME),      # Extract month
                          Day = day(DATETIME),          # Extract day
                          Hour = hour(DATETIME),        # Extract hour
                          Minute = minute(DATETIME)) %>%  # Extract minute
      mutate(TimeOfDay = Hour + Minute / 60,   # Time of the day (0 - 24)
             WeekOfMonth = factor(week(DATETIME) - week(floor_date(DATETIME, "month")) + 1, levels = c(1,2,3,4,5,6)),
             DayOfWeek = factor(wday(DATETIME, label=TRUE), levels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")))
  } else {
    warning("DATETIME column not found in the dataframe.")
  }
  return(df)
}
```

```{r}
summary(data_demand_nsw)
df_demand_nsw <- wrangle_demand_data(data_demand_nsw)
head(df_demand_nsw)
```

```{r yearly-demand-plot, fig.cap="Total Demand Over Time."}
#The time series plot shows the fluctuation of electricity demand in New South Wales over 11 years. A clear seasonal pattern is observed, with peaks during winter and summer months, reflecting higher heating and cooling needs.

# Calculate sequence of years
year_sequence <- seq(from = floor_date(min(df_demand_nsw$DATETIME), "year"),
                     to = ceiling_date(max(df_demand_nsw$DATETIME), "year"),
                     by = "1 year")

# Plot data with vertical lines
df_demand_nsw %>% 
  ggplot(aes(x = DATETIME, y = TOTALDEMAND)) +
  geom_line() +
  geom_vline(data = data.frame(x = year_sequence), 
             aes(xintercept = as.numeric(x)), 
             linetype = "dotted", color = "orange") +
  scale_x_datetime(date_breaks = '1 year', date_labels = '%Y') +
  labs(x = "Year", y = "Total Demand", title = "Total Demand Over Time") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) 

```

```{r monthly-demand-plot, message=FALSE, fig.cap="Weekly Pattern of Demand By Month."}
#This set of line graphs illustrates the average electricity demand throughout a typical week in each month of the year, with lines representing different weeks within each month. 
df_demand_nsw_monthly <- df_demand_nsw %>% 
  group_by(Month, WeekOfMonth, TimeOfDay) %>% 
  summarise(MeanDemand = mean(TOTALDEMAND)) %>% 
  ungroup()

df_demand_nsw_monthly %>% 
  ggplot(aes(x = TimeOfDay, y = MeanDemand)) +
  geom_line(aes(color = WeekOfMonth)) +
  facet_wrap(~ Month, ncol = 3) + 
  scale_x_continuous(name = "Time of Day", breaks = seq(0, 24, by = 3), labels = function(x) sprintf("%02d:00", x)) +
  scale_y_continuous(name = "Mean Total Demand") +
  labs(title = "Weekly Pattern of Electricity Demand by Month") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        strip.text.x = element_text(size = 6), legend.position = "right", plot.title = element_text(hjust = 0.5))

```
There is a consistent daily pattern across all months, with demand increasing during the day and peaking in the early evening before dropping off at night. This likely reflects increased household and business activity during daylight hours and reduced activity overnight. 

There are some differences in the daily patterns across the months, which seem to be influenced by seasonal changes.

Within each month, the weekly patterns remain relatively consistent, although there are slight variations in demand from week to week. This consistency indicates a stable electricity usage pattern within a month across different weeks.

```{r seasonaly-demand-plot, fig.cap="Demand by Hour Across Seasons."}
# This plot prints the seasonal demand of electricity for entire data
# Create a new column 'Season' based on the month
df_demand_nsw_seasonly <- df_demand_nsw %>% 
  mutate(Season = case_when(
    Month %in% c(12, 1, 2) ~ "Summer",
    Month %in% c(3, 4, 5) ~ "Autumn",
    Month %in% c(6, 7, 8) ~ "Winter",
    Month %in% c(9, 10, 11) ~ "Spring",
    TRUE ~ NA_character_))

# Plot boxplots
df_demand_nsw_seasonly %>% 
  ggplot(aes(x = factor(Hour), y = TOTALDEMAND)) +  # Specify x and y variables
  geom_boxplot(aes(fill=Season)) +  # Add boxplots, fill by season
  facet_wrap(~Season, scales = "free_y") +  # Facet by season, with independent y-axes
  labs(title = "Electricity Demand by Hour Across Seasons",  # Set plot title and axis labels
       x = "Hour of Day",
       y = "Total Demand") +
  scale_x_discrete(name = "Time of Day", breaks = seq(0, 24, by = 2)) +  # Customize x-axis
  theme_minimal() +  # Apply minimal theme
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5),  # Improve readability of x-axis labels
        plot.title = element_text(hjust = 0.5))  # Center plot title


```

Autumn (Red) and Spring (Green): These seasons exhibit relatively consistent electricity demand with slight increases in the evening hours, likely reflective of the typical residential consumption patterns during these milder months.

Summer (Cyan): The demand profile during Summer shows a peak in midday to afternoon demand, establishing the anticipated increase in electricity use for cooling as the day reaches its highest temperatures.

Winter (Purple): Winter shows distinct peaks in demand in the early morning and evening, likely due to heating requirements during the colder parts of the day, which aligns with our initial observations of potential seasonal impacts on electricity use.

```{r ,message=FALSE, fig.cap="Mean Demand by Hour of Each Day of Week."}
# This code snippet segments the demand data by day of the week and time of day and calculates the mean demand for each combination. Then, it creates a line plot to visualize the mean total demand by time of day, with each line representing a different day of the week.

df_demand_nsw_weekly <- df_demand_nsw %>% 
  group_by(DayOfWeek, TimeOfDay) %>% 
  summarise(MeanDemand = mean(TOTALDEMAND, na.rm = TRUE))

df_demand_nsw_weekly %>% ggplot(aes(x = TimeOfDay, y = MeanDemand)) +
  geom_line(aes(color = DayOfWeek)) +
  scale_x_continuous(name = "Time of Day", breaks = seq(0, 24, by = 1), labels = function(x) sprintf("%02d:00", x)) +
  scale_y_continuous(name = "Mean Total Demand") +
  scale_color_brewer(palette = "Dark2") +
  labs(title = "Mean demand against day-of-week") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 0.5), plot.title = element_text(hjust = 0.5))


```
Weekday Demand (Monday to Friday): There is a pronounced double-peaked pattern on weekdays, with electricity demand rising sharply in the morning, tapering slightly midday, and then peaking again in the early evening. This likely reflects routine domestic and commercial activities, such as people getting ready for work and school in the morning and then returning home and using various appliances in the evening.
Weekend Demand (Saturday and Sunday): The weekend pattern, while still showing two peaks similar to the weekdays, has less overall demand.
Overall Trends: Across all days, there is a significant drop in demand late at night, when most residential and commercial activities cease, until early morning when the demand begins to rise with the start of a new day.

```{r Daily-Demand-plot, message=FALSE, fig.cap="Average Daily Demand."}
# This code snippet calculates the average daily demand by grouping the data by month and day. It then creates a line plot to visualize the trend of average daily demand over time, with smoothed lines representing the trend.

df_demand_nsw_daily <- df_demand_nsw %>%
  group_by(Month, Day) %>% 
  summarise(Avg_daily_demand = mean(TOTALDEMAND)) %>% 
  mutate(DATE = make_date(year=2012, month=Month, day=Day),
         MonthName = month(DATE, label=TRUE))

df_demand_nsw_daily %>% 
  ggplot(aes(x = DATE, y = Avg_daily_demand)) +
  geom_line() + # Draw lines between points
  geom_point() + # Add points for each data point
  geom_smooth() +
  scale_x_date(date_breaks = "1 month", date_labels = "%b") + # Customize x-axis labels
  labs(x = "Month", y = "Avg demand", title = "Average daily demand") +
  theme_minimal() + # Use a minimal theme
  theme(plot.title = element_text(hjust = 0.5)
)


```
We see some extremes lows, seems to be the effect of holidays, will dive into that later.

## Temperature

```{r}

summary(data_temperature_nsw)
df_temperature_nsw <- wrangle_demand_data(data_temperature_nsw)
head(df_temperature_nsw)


```

The temperature data does not have constant update times; to better align it with the demand data, we categorize the temperature readings into two groups: readings from 45 minutes past the hour to 14 minutes past the next hour are aligned as the 0-minute mark, and readings from 15 to 44 minutes past the hour are aligned as the 30-minute mark. This method ensures that the temperature data corresponds closely with the demand data's half-hour sampling interval.

```{r}

df_temperature_nsw <- df_temperature_nsw %>%
  mutate(out_of_line = !(((Minute >= 45) | (Minute <= 14)) | ((Minute >= 15) & (Minute <= 44))))

# Count how many intervals are out of the expected line
sum_out_of_line <- sum(df_temperature_nsw$out_of_line, na.rm = TRUE)

# Print the count of out-of-line intervals
print(sum_out_of_line)

```

There are 16089 numbers of out of line data.

```{r}

df_aligned_temperature_nsw <- df_temperature_nsw %>%
  # Create a new column for the aligned timestamp
  mutate(DATETIME = DATETIME %>%
           # Round down or up to the nearest 30-minute mark
           floor_date(unit = "30 minutes") %>%
           # Adjust the rounding (55-05 -> 0, 25-35 -> 30)
           {if_else(minute(.) >= 45 | minute(.) <= 14, . - minutes(minute(.)), .)} %>%
           {if_else(minute(.) >= 15 & minute(.) <= 44, . + minutes(30 - minute(.)), .)}) %>%
  # Group by this new column
  group_by(DATETIME) %>% 
  # Calculate the mean temperature for each group
  summarise(TEMPERATURE = mean(TEMPERATURE))

```

```{r}

head(df_aligned_temperature_nsw)

```

```{r Temperature-plot, fig.cap="Temperature Over Time."}

#This code produces a plot with temperature data over time, blue vertical lines indicating the start of each year.

df_aligned_temperature_nsw %>% 
  ggplot(aes(x = DATETIME, y = TEMPERATURE)) +
  geom_line() +
  geom_vline(data = data.frame(x = seq(from = floor_date(min(df_temperature_nsw$DATETIME), "year"),
                                       to = ceiling_date(max(df_temperature_nsw$DATETIME), "year"),
                                       by = "1 year")), 
             aes(xintercept = as.numeric(x)), 
              color = "blue") +  # Change the color to red
  scale_x_datetime(date_breaks = "1 year", date_labels = "%Y") +
  labs(x = "Year", y = "Temperature", title = "Temperature variation over time") +
  theme(plot.title = element_text(hjust = 0.5)
)

```
The data shows strong seasonality in the variation.

## Forecast Data

The lastest LASTCHANGED forecastdemand for each DATETIME is selected.
```{r}
head(data_forecastdemand_nsw)

df_forecastdemand_nsw <- data_forecastdemand_nsw %>%
  group_by(DATETIME) %>%
  filter(LASTCHANGED == max(LASTCHANGED)) 

```

# Bivariate Analysis 

## Holiday vs non-Holidays impact on Demand

```{r demand-holiday-plot, fig.cap="Effect of Holidays on Demand."}

df_demand_nsw <- df_demand_nsw %>% 
  mutate(Date = date(DATETIME)) %>% 
  mutate(
    IsHoliday = case_when(
      date(DATETIME) %in% holidays_nsw$Date ~ 'Holiday',
      TRUE ~ 'Non-Holiday'
    ))

# Boxplot to compare electricity demand on holidays vs non-holidays
df_demand_nsw %>% 
  ggplot(aes(x = IsHoliday, y = TOTALDEMAND)) +
  geom_boxplot(aes(fill=IsHoliday)) +
  labs(title = "Demand of Holidays vs Non-Holidays",
       x = "", y = "Demand") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

```

The box plot compares electricity demand on holidays versus non-holidays. It appears that the median electricity demand on holidays is lower than on non-holidays. There are several extreme values that deviate from the typical ranges on both types of days.

## Demand vs Temperature
```{r}

# Inner join to get only matched data
df_nsw <- inner_join(df_demand_nsw, df_aligned_temperature_nsw, by = "DATETIME")

# Outer join to find unmatched data
unmatched_demand <- anti_join(df_demand_nsw, df_aligned_temperature_nsw, by = "DATETIME")
unmatched_temperature <- anti_join(df_aligned_temperature_nsw, df_demand_nsw, by = "DATETIME")

# Combine the unmatched data into one tibble
excluded_data <- bind_rows(unmatched_demand, unmatched_temperature)

# Optionally, add a column to indicate the source of each row in the excluded data
excluded_data <- excluded_data %>%
  mutate(Source = ifelse(is.na(TEMPERATURE), "Demand Only", "Temperature Only"))

nrow(excluded_data)

unique(excluded_data$Source)

```

Correlation testing
```{r}

cor.test(df_nsw$TEMPERATURE, df_nsw$TOTALDEMAND, use = "pearson")

```
Pearson correlation test: p-value is small reject null-hypothesis, it is statistically significant to say the true correlation of demand and temperature is not equal to 0.

```{r demand-temperature-plot, fig.cap="Relationship between Demand and Temperature."}

df_nsw %>% 
  ggplot(aes(x = TEMPERATURE, y = TOTALDEMAND, color = "Data Points")) +  # Set color aesthetic
  geom_point(color = "red") +  
  labs(title = "Temperature and electricity demand relationship",
       x = "Temperature",
       y = "Electricity Demand") +
theme(plot.title = element_text(hjust = 0.5))

```

## Converts to Daily Data

```{r}

df_solar_expo_nsw <- solar_expo_nsw %>% 
  mutate(Solar_exposure = `Daily global solar exposure (MJ/m*m)`,
         DATE = make_date(year = Year, month = Month, day = Day)) %>% 
  filter(Year >= 2010 & Year <= 2022) %>% 
  select(DATE, Solar_exposure) 

df_solar_expo_nsw <- df_solar_expo_nsw %>% 
  filter(!is.na(Solar_exposure))
  
df_rainfall_nsw <- rainfall_nsw %>% 
  mutate(Rainfall_amount = `Rainfall amount (millimetres)`,
         DATE = make_date(year = Year, month = Month, day = Day)) %>% 
  filter(Year >= 2010 & Year <= 2022) %>% 
  select(DATE, Rainfall_amount) 

median_rainfall <- median(df_rainfall_nsw$Rainfall_amount, na.rm = TRUE)
# Impute NA values in Rainfall_amount with the median

df_rainfall_nsw <- df_rainfall_nsw %>%
  mutate(Rainfall_amount = ifelse(is.na(Rainfall_amount), median_rainfall, Rainfall_amount))

```

```{r}

# Converts to daily data to match solar exposure and rainfall 
df_nsw_daily <- df_nsw %>% 
  mutate(DATE = as.Date(DATETIME)) %>% 
  group_by(DATE) %>% 
  summarise(Daily_Mean_Temperature = mean(TEMPERATURE, na.rm = TRUE),
            Daily_Max_Temperature = max(TEMPERATURE, na.rm = TRUE),
            Daily_Min_Temperature = min(TEMPERATURE, na.rm = TRUE),
            Daily_Mean_Demand = mean(TOTALDEMAND, na.rm = TRUE),
            Daily_Max_Demand = max(TOTALDEMAND, na.rm = TRUE),
            Daily_Min_Demand = min(TOTALDEMAND, na.rm = TRUE)) %>% 
    ungroup() %>% 
    mutate(WeekOfMonth = factor(week(DATE) - week(floor_date(DATE, "month")) + 1, levels = c(1,2,3,4,5,6)),
           DayOfWeek = factor(wday(DATE, label=TRUE), levels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")))
  
df_nsw_daily <- inner_join(df_nsw_daily, df_solar_expo_nsw, by = 'DATE')

```

```{r}

# Transform the data from wide to long format
df_nsw_daily_long <- df_nsw_daily %>%
  pivot_longer(cols = c("Daily_Mean_Demand", "Daily_Max_Demand", "Daily_Min_Demand"),
               names_to = "Demand_Type",
               values_to = "Demand") %>% 
    pivot_longer(cols = c("Daily_Mean_Temperature", "Daily_Max_Temperature", "Daily_Min_Temperature"),
               names_to = "Temperature_Type",
               values_to = "Temperature")

```

## Solar Exposure vs Daily Demand

```{r solarexpo-demand-plot, fig.cap="Relationship between Solar Exposure and Daily Demand."}
# Plotting with faceting
ggplot(df_nsw_daily_long, aes(x = Solar_exposure, y = Demand)) +
  geom_point() +
  geom_smooth(se = FALSE, method = "loess") + 
  ggtitle("Solar Exposure vs Daily Demand in NSW") +
  xlab("Solar Exposure") +
  ylab("Demand") +
  facet_wrap(~Demand_Type, scales = "free_y") + # Create a separate panel for each Demand_Type
  theme_minimal()

```

## Solar Exposure vs Daily Temperature

```{r solarexpo-temperature-plot, warning=FALSE, fig.cap="Relationship between Solar Exposure and Daily Temperature"}

# Plotting with faceting
ggplot(df_nsw_daily_long, aes(x = Solar_exposure, y = Temperature)) +
  geom_point() +
  geom_smooth(se = FALSE, method = "loess") + 
  ggtitle("Solar Exposure vs Temperature in NSW") +
  xlab("Solar Exposure") +
  ylab("Temperature") +
  facet_wrap(~Temperature_Type, scales = "free_y") + 
  theme_minimal()

```

```{r}

cor.test(df_nsw_daily$Daily_Mean_Demand, df_nsw_daily$Solar_exposure, use="pearson")
cor.test(df_nsw_daily$Daily_Max_Demand, df_nsw_daily$Solar_exposure, use="pearson")
cor.test(df_nsw_daily$Daily_Min_Demand, df_nsw_daily$Solar_exposure, use="pearson")
cor.test(df_nsw_daily$Daily_Mean_Temperature, df_nsw_daily$Solar_exposure, use="pearson")
cor.test(df_nsw_daily$Daily_Max_Temperature, df_nsw_daily$Solar_exposure, use="pearson")
cor.test(df_nsw_daily$Daily_Min_Temperature, df_nsw_daily$Solar_exposure, use="pearson")

```

## Rainfall vs Daily Demand

```{r rainfall-demand-plot, message=FALSE, warning=FALSE, fig.cap="Relationship between Rainfall and Daily Demand."}

df_nsw_daily <- inner_join(df_nsw_daily, df_rainfall_nsw)
df_nsw_daily_long <- inner_join(df_nsw_daily_long, df_rainfall_nsw)

# Plotting with faceting
ggplot(df_nsw_daily_long, aes(x = Rainfall_amount, y = Demand)) +
  geom_point() +
  geom_smooth(se = FALSE, method = "loess") + 
  ggtitle("Daily Demand vs Rainfall Amount in NSW") +
  xlab("Rainfall Amount") +
  ylab("Electricity Demand") +
  facet_wrap(~Demand_Type, scales = "free_y") + # Create a separate panel for each Demand_Type
  theme_minimal()

```

## Forecast(Benchmark) vs Demand

```{r}

df_benchmark <- inner_join(df_forecastdemand_nsw, df_demand_nsw, by='DATETIME')

```

Taking random samples from 2017.
```{r}

df_benchmark <- df_benchmark %>% 
  mutate(DATE = as.Date(DATETIME))

dates_2017 <- unique(df_benchmark %>% 
  filter(Year == 2017))

set.seed(123)

# Sample 6 random dates
sampled_dates <- sample(dates_2017$DATE, 6)

sampled_df <- df_benchmark %>%
  filter(DATE %in% sampled_dates)

```


```{r benchmark-demand-plot, fig,cap="Benchmark versus Actual Demand."}

ggplot(sampled_df, aes(x = DATETIME)) + 
  geom_line(aes(y = FORECASTDEMAND, color = "Forecast Demand"), linetype = "dashed", size = 1.2) + 
  geom_line(aes(y = TOTALDEMAND, color = "Total Demand"), size = 1.2) +
  geom_point(aes(y = FORECASTDEMAND, color = "Forecast Demand"), size = 2, alpha = 0.5) + 
  geom_point(aes(y = TOTALDEMAND, color = "Total Demand"), size = 2, alpha = 0.5) +
  labs(title = "Benchmark Forecast vs Total Demand on Random Days in 2017",
       x = "Hour of the Day",
       y = "Demand",
       color = "Legend") +
  scale_color_manual(values = c("Forecast Demand" = "red", "Total Demand" = "blue")) +
  scale_x_datetime(date_labels = "%H", date_breaks = "2 hours") +
  theme_minimal() +
  theme(legend.title = element_blank(),
        legend.position = "bottom",
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~DATE, ncol = 3, scales = "free_x")

```

The benchmark seems to be off, it does not accurately forecast the actual demand.

```{r}

df_forecastdemand_nsw_new <- df_forecastdemand_nsw %>% 
  mutate(DATETIME = DATETIME - hours(10))

df_max2 <- inner_join(df_forecastdemand_nsw_new, df_demand_nsw, by='DATETIME')

```

```{r}

df_max2 <- df_max2 %>% 
  mutate(DATE = as.Date(DATETIME))

dates_2017_max2 <- unique(df_max2 %>% 
  filter(Year == 2017))

set.seed(123)

# Sample 6 random dates
sampled_dates2 <- sample(dates_2017_max2$DATE, 6)


sampled_df_max2 <- df_max2 %>%
  filter(DATE %in% sampled_dates2)

```

```{r shifted-benchmark-demand-plot, fig.cap="10-hours Backwards Benchmark versus Actual Demand."}

ggplot(sampled_df_max2, aes(x = DATETIME)) + 
  geom_line(aes(y = FORECASTDEMAND, color = "Forecast Demand"), linetype = "dashed", size = 1.2) + 
  geom_line(aes(y = TOTALDEMAND, color = "Total Demand"), size = 1.2) +
  geom_point(aes(y = FORECASTDEMAND, color = "Forecast Demand"), size = 2, alpha = 0.5) + 
  geom_point(aes(y = TOTALDEMAND, color = "Total Demand"), size = 2, alpha = 0.5) +
  labs(title = "Benchmark Forecast (10-hours backward shifted) vs Total Demand on Random Days in 2017",
       x = "Hour of the Day",
       y = "Demand",
       color = "Legend") +
  scale_color_manual(values = c("Forecast Demand" = "red", "Total Demand" = "blue")) +
  scale_x_datetime(date_labels = "%H", date_breaks = "2 hours") +
  theme_minimal() +
  theme(legend.title = element_blank(),
        legend.position = "bottom",
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~DATE, ncol = 3, scales = "free_x")

```

After adjusting the DATETIME of the benchmark by delaying it by 10 hours, we have observed a significant improvement in accuracy. Consequently, this revised benchmark will now serve as the new standard for comparing against our model.

## Correlation Heatmap - Demand
```{r cormap-demand, fig.cap="Correlation Map of Demand and Time."}

cor_df <- df_demand_nsw %>% 
  select(-DATETIME, -REGIONID, -Date) %>% 
  mutate(IsHoliday = case_when(IsHoliday == 'Holiday' ~ 0, TRUE ~ 1)) %>% 
  mutate(across(everything(), as.numeric)) %>% 
  select(-Day,-Hour)

cor_matrix <- cor(cor_df)

corrplot(cor_matrix, method = "color", type = "full", order = "hclust",
         tl.col = "black", tl.srt = 45, 
         addCoef.col = "black") 

```

## Correlation Heatmap - Daily data

```{r cormap-daily-data, fig.cap="Correlation Map of Daily Data."}

cor_df2 <- df_nsw_daily %>% 
  select(-DATE, -WeekOfMonth, -DayOfWeek) 

cor_matrix2 <- cor(cor_df2)


corrplot(cor_matrix2, method = "color", type = "full", order = "hclust",
         tl.col = "black", tl.srt = 45, 
         addCoef.col = "black") 

```

## STL decomposition

```{r}

electricity_ts <- df_demand_nsw %>%
  select(DATETIME, TOTALDEMAND) %>% 
  as_tsibble(index = DATETIME) 

```

```{r stl-plot, fig.cap="STL Decomposition."}

decomposed <- electricity_ts %>%
  model(STL = STL(TOTALDEMAND ~ season(window = "periodic"),
                  robust = TRUE)) %>%
  components()

decomposed %>% autoplot()

```

The STL decomposition shows multiple seasonality, hourly, daily, weekly and yearly.

```{r}

# Extracting seasonal components
seasonal_components <- decomposed %>%
  select(trend, season_hour , season_day, season_week, season_year ) 

```

```{r}

lm_model <- decomposed %>%
  model(lm = TSLM(TOTALDEMAND  ~ trend + season_hour + season_day + season_week + season_year))

report(lm_model)

```

Hourly seasonality seem to be weak.

## Autocorrelation

```{r}

dfts_demand_nsw <- as_tsibble(df_demand_nsw %>% select(DATETIME, TOTALDEMAND), index=DATETIME)

```

### Non-Seasonal Differencing
```{r, message=FALSE}

dfts_demand_nsw %>% features(TOTALDEMAND, unitroot_kpss)

dfts_demand_nsw %>%
  mutate(diff_TOTALDEMAND = difference(TOTALDEMAND)) %>% 
  features(diff_TOTALDEMAND, unitroot_kpss)

```
### Seasonal Differencing

```{r}

# 0.01 Non-stationary
dfts_demand_nsw %>% 
  features(TOTALDEMAND, unitroot_kpss) 

# 0.1 Stationary after first order differencing
dfts_demand_nsw %>% 
  mutate(diff_TOTALDEMAND = difference(TOTALDEMAND, 48)) %>% 
  features(diff_TOTALDEMAND, unitroot_kpss) 

```
The KPSS test suggests that after first-order differencing the model is stationary. (Both non-seasonal and seasonal)

### ACF and PACF after first-order differencing

```{r diff-ACF, fig.cap="Seasonally Differenced ACF plot of Demand Data."}

dfts_demand_nsw %>% 
  mutate(diff_TOTALDEMAND = difference(TOTALDEMAND, 48)) %>% 
  ACF(diff_TOTALDEMAND, lag_max = 336*2) %>% 
  autoplot()
```

```{r diff-PACF, fig.cap="Seasonally Differenced PACF plot of Demand Data."}
dfts_demand_nsw %>% 
  mutate(diff_TOTALDEMAND = difference(TOTALDEMAND, 48)) %>% 
  PACF(diff_TOTALDEMAND, lag_max = 336) %>% 
  autoplot()

```

# Modelling

## Data preperation for modelling

Combining the data into a single dataframe
```{r}

df_lj <- left_join(df_demand_nsw, df_aligned_temperature_nsw, by='DATETIME')
check_for_na(df_lj)

```

```{r}

# Fill NA values
df_lj$TEMPERATURE <- na.spline(df_lj$TEMPERATURE)

```
Fills NA values with na.spline()

```{r}

# Uses the latest last change date
df_forecastdemand_nsw <- data_forecastdemand_nsw %>% 
  mutate(DATETIME = as_datetime(DATETIME, tz='AUstralia/Brisbane')) %>% 
  mutate(DATETIME = DATETIME - hours(10)) %>%  # Shift DATETIME by 10 hours earlier
  group_by(DATETIME) %>%
  filter(LASTCHANGED == max(LASTCHANGED)) %>%
  summarize(FORECASTDEMAND = mean(FORECASTDEMAND), .groups = 'drop') %>% 
  ungroup() 

```

```{r}

df_nsw <- left_join(df_lj, df_forecastdemand_nsw, by='DATETIME')
check_for_na(df_nsw)

```

```{r}

df <- df_nsw %>% 
  select(DATETIME, TOTALDEMAND, TEMPERATURE, FORECASTDEMAND) %>% 
  mutate(
    TEMPERATURE_SQ = TEMPERATURE^2
  ) %>%
  as_tsibble(index = DATETIME)

```

## Model Comparison

```{r}

start_date <- ymd("2015-06-01")
end_date <- ymd("2018-06-30")

dfts_nsw <- df %>% 
  select(DATETIME, TOTALDEMAND, TEMPERATURE, FORECASTDEMAND,TEMPERATURE_SQ) %>% 
  as_tsibble(index = DATETIME) %>% 
  filter(DATETIME >= start_date & DATETIME <= end_date)

```

```{r}

n <- nrow(dfts_nsw)

train_set <- dfts_nsw[1:(n-48), ]
test_set <- dfts_nsw[(n-47):n, ]

```

```{r tsdisplay, fig.cap="Residuals, ACF and PACF of Double Differenced Demand Data."}

train_set %>% 
  gg_tsdisplay(
    difference(difference(TOTALDEMAND, lag = 48), lag = 1), 
    plot_type = 'partial', 
    lag_max = 48*7
  ) +
  labs(title = 'Double Differenced', y = '')

```

### Model Training on Train Set
```{r model-comparison-training}

fit <- train_set %>% 
  model(
    arima_fr = ARIMA(TOTALDEMAND ~ pdq(d=1) + PDQ(0,0,0) +
                        fourier(period = 48, K = 3) +     # Daily 
                        fourier(period = 48*7, K = 3)),   # Weekly
    sarima_212210 = ARIMA(TOTALDEMAND ~ pdq(2,1,2) + PDQ(2,1,0, period=48)),
    
    sarima_212210_fr = ARIMA(TOTALDEMAND ~ pdq(2,1,2) + PDQ(2,1,0, period=48) +
                        fourier(period = 48*7, K = 3)), # Weekly
    
    sarima_auto_temp = ARIMA(TOTALDEMAND ~ pdq(d=1) + PDQ(D=1, period=48) + TEMPERATURE + TEMPERATURE_SQ),
    
    sarima_212210_temp_fr = ARIMA(TOTALDEMAND ~ pdq(2,1,2) + PDQ(2,1,0, period=48) + 
                                TEMPERATURE + TEMPERATURE_SQ +
                                fourier(period = 48*7, K = 3))    # Weekly
  )

```

```{r}
## Show all models fitted

fit %>% pivot_longer(everything(), names_to = "Model name", values_to = "Orders")

## Order by smallest BIC
glance(fit) %>% arrange(BIC) %>% select(.model:BIC)

```
SARIMA_212210[48] with fourier terms and temperature gives the best AIC and BIC values, whereas ARIMA with fourier terms performs the worst.

### Model Forecast on Test Set
```{r}

# Forecasting
forecasts <- fit %>%
  forecast(test_set)

```

```{r}

forecasts %>% accuracy(df) %>% arrange(RMSE, 'desc')

```
sarima_212210_fr shows a better performance than sarima_212210_temp_fr by small margin. arima_fr performs worst again on the test set.

```{r model-comparison-plot, fig.cap="Comparison of Model Forecasts."}

forecasts %>% autoplot(tail(dfts_nsw, 96), level=NULL, size=1.2, alpha=0.9)

```

## Final Model

<b>The ARIMA(2,1,2)(2,1,0) model with temperature and Fourier terms for seasonal adjustment is structured as follows:</b>

$$
(1 - \phi_1 B - \phi_2 B^2)(1 - \Phi_1 B^{48} - \Phi_2 B^{96}) (1 - B)^1 (1 - B^{48}) y_t = \\
(1 + \theta_1 B + \theta_2 B^2) \epsilon_t + \beta_1 x_{1,t} + \beta_2 x_{2,t} + \beta_3 \sum_{k=1}^3 \left( \alpha_k \cos\left(\frac{2 \pi k t}{336}\right) + \gamma_k \sin\left(\frac{2 \pi k t}{336}\right) \right)
$$


**Where:**

- \(y_t\) is the TOTALDEMAND at time \(t\),
- \(\phi_1, \phi_2\) are the coefficients for the non-seasonal AR terms,
- \(\Phi_1, \Phi_2\) are the coefficients for the seasonal AR terms,
- \(\theta_1, \theta_2\) are the coefficients for the non-seasonal MA terms,
- \(B\) is the backshift operator, \(B^k y_t = y_{t-k}\),
- \(x_{1,t}\) is the TEMPERATURE at time \(t\),
- \(x_{2,t}\) is the TEMPERATURE squared at time \(t\),
- The Fourier terms for \(K=3\) and period \(336\) (hourly data over a week) are represented by cosine and sine functions where \(\alpha_k\) and \(\gamma_k\) are the coefficients for cosine and sine components respectively, \(t\) is the time index,
- \(\beta_1, \beta_2, \beta_3\) are the coefficients for the TEMPERATURE, TEMPERATURE squared, and Fourier terms respectively,
- \(\epsilon_t\) is the error term.

**Differencing:**

- \((1 - B)^1\) and \((1 - B^{48})\) represent the first and seasonal differencing, making the series stationary.

**Notes:**

- This model includes both non-seasonal and seasonal components, capturing patterns at different time scales.
- Fourier terms are used to model weekly seasonal patterns that are not captured by simple seasonal differencing.

We utilizes the roll forward validation method, to forecast 24 hours ahead 5 consecutive days.
```{r}

plan(multisession)

```


```{r fianl-model-training}

initial_train_end <- nrow(dfts_nsw) - 48 * 5  # Training data ends 5 days before the dataset's end
forecast_horizon <- 48  # Forecasting one day ahead (48 half-hour intervals)
forecasts_list <- vector("list", 5)  # Store each forecast

future_list <- vector("list", 5)  # List to store futures

for (i in 1:5) {
  future_list[[i]] <- future({
    train_data <- dfts_nsw[1:(initial_train_end + (i - 1) * forecast_horizon), ]
    
    cat(sprintf("Fit %d starting: %s\n", i, Sys.time()))
    
    fit_arima <- train_data %>%
      model(sarima_212210_temp_fr = ARIMA(TOTALDEMAND ~ pdq(2, 1, 2) + PDQ(2, 1, 0, period = 48) +
                                          TEMPERATURE + TEMPERATURE_SQ +
                                          fourier(K = 3, period = 48*7)))
    
    cat(sprintf("Fit %d done: %s\n", i, Sys.time()))
    
    test_set <- dfts_nsw[(initial_train_end + (i - 1) * forecast_horizon + 1):(initial_train_end + i * forecast_horizon), ]
    forecasts <- forecast(fit_arima, new_data = test_set)
    return(forecasts)
  })
}

# Collecting results from futures
for (i in 1:5) {
  forecasts_list[[i]] <- value(future_list[[i]])
}

# Combine forecasts into a single tsibble for plotting or evaluation
combined_forecasts <- bind_rows(forecasts_list) %>% as_tibble()

```


```{r}

# Extract forecast means
forecast_means <- combined_forecasts %>%
  select(.mean) %>%
  pull()

# Create a time series object from the forecasted means
pred_arima_ts <- ts(forecast_means, start = c(ceiling((initial_train_end + 1) / 48), (initial_train_end + 1) %% 48), frequency = 48)

# Actual data corresponding to the forecast periods
actual_indices <- seq(initial_train_end + 1, nrow(dfts_nsw))
actual_data <- dfts_nsw$TOTALDEMAND[actual_indices]

# Calculate accuracy metrics
accuracy_metrics <- accuracy(pred_arima_ts, actual_data)
print(accuracy_metrics)

```

```{r final-model-plot, fig.cap="Forecast of SARIMA_212210[48] with Weekly Fourier and Temperature." }

# Create a data frame for plotting
plot_data <- data.frame(
  Time = seq_along(actual_data),
  Actual = actual_data,
  Forecast = as.numeric(pred_arima_ts)
)

# Creating the plot
p <- ggplot(plot_data, aes(x = Time)) +
  geom_line(aes(y = Actual, colour = "Actual"), size = 1.5) +
  geom_line(aes(y = Forecast, colour = "Forecast"), size = 1.5) +
  scale_color_manual(values = c("Actual" = "black", "Forecast" = "red")) +
  labs(title = "SARIMAX Forecast vs Actual Data",
       x = "Time (in 48 half-hour intervals per day)",
       y = "Total Demand",
       colour = "Legend") +
  theme_minimal()

# Display the plot
print(p)

```

## Comparison with Benchmark

```{r}

benchmark_test <- dfts_nsw[initial_train_end:nrow(dfts_nsw),]

# Calculate accuracy metrics
benchmark_accuracy <- accuracy(benchmark_test$FORECASTDEMAND, benchmark_test$TOTALDEMAND)
print(benchmark_accuracy)

benchmark_test <- benchmark_test %>% 
  mutate(Benchmark = FORECASTDEMAND)

```

```{r}

full_data <- benchmark_test %>%
  left_join(combined_forecasts, by = "DATETIME") %>%
  rename(Forecast = `.mean`)

```

```{r}

# Merge the data frames by 'Metric'
comparison_table <- rbind(accuracy_metrics, benchmark_accuracy)

row.names(comparison_table) <- rep(c("Our Forecast", "Benchmark"), each = nrow(accuracy_metrics))
# Display the comparison table
print(comparison_table)

```

```{r fianl-model-benchmark-comparison-plot, fig.cap="Comparison of Benchmark and Our Final Model against the Actual Demand."}

# Create the plot
plot <- ggplot(full_data, aes(x = DATETIME)) +
  geom_line(aes(y = TOTALDEMAND.x, color = "Actual Demand"), size = 3) +
  geom_line(aes(y = Benchmark, color = "Benchmark"), size = 1) +
  geom_line(aes(y = Forecast, color = "Forecast"), size = 1.5) +
  scale_color_manual(values = c("Actual Demand" = "black", 
                                "Benchmark" = "blue", 
                                "Forecast" = "red")) +
  labs(title = "Comparison of Forecast, Benchmark with Actual Data",
       x = "Datetime",
       y = "Demand",
       color = "Legend") +
  theme_minimal()

# Display the plot
print(plot)

```
# Conclusion

The analytical journey within this report culminates in the validation of a SARIMA model incorporating weekly Fourier terms and temperature data as the most suitable for forecasting electricity demand in NSW. This model excels in capturing significant seasonal trends and responding to temperature fluctuations, thereby markedly improving forecast precision. Our findings demonstrate the potential of this model to robustly predict demand fluctuations, supporting more accurate and efficient operational planning. Future directions for this research include integrating additional predictive variables and further refining the model to enhance its adaptability and accuracy in response to changing demand patterns. This document details the methodologies and insights obtained, providing a foundation for further research and application in the field of energy demand forecasting.

